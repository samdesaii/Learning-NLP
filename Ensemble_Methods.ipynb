{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wmpodQkfokCX"
      },
      "source": [
        "## Import packages\n",
        "Make sure you installed ***eli5***, ***sklearn***, ***matplotlib*** and ***numpy*** if you use your local machine"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "oQWB7ST6fYj6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Defaulting to user installation because normal site-packages is not writeable\n",
            "Collecting eli5\n",
            "  Downloading eli5-0.13.0.tar.gz (216 kB)\n",
            "  Installing build dependencies: started\n",
            "  Installing build dependencies: finished with status 'done'\n",
            "  Getting requirements to build wheel: started\n",
            "  Getting requirements to build wheel: finished with status 'done'\n",
            "  Preparing metadata (pyproject.toml): started\n",
            "  Preparing metadata (pyproject.toml): finished with status 'done'\n",
            "Collecting attrs>17.1.0 (from eli5)\n",
            "  Downloading attrs-25.1.0-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: jinja2>=3.0.0 in c:\\users\\shyam\\appdata\\roaming\\python\\python312\\site-packages (from eli5) (3.1.4)\n",
            "Requirement already satisfied: numpy>=1.9.0 in c:\\users\\shyam\\appdata\\roaming\\python\\python312\\site-packages (from eli5) (2.1.2)\n",
            "Requirement already satisfied: scipy in c:\\users\\shyam\\appdata\\roaming\\python\\python312\\site-packages (from eli5) (1.14.1)\n",
            "Requirement already satisfied: six in c:\\users\\shyam\\appdata\\roaming\\python\\python312\\site-packages (from eli5) (1.16.0)\n",
            "Requirement already satisfied: scikit-learn>=0.20 in c:\\users\\shyam\\appdata\\roaming\\python\\python312\\site-packages (from eli5) (1.5.2)\n",
            "Collecting graphviz (from eli5)\n",
            "  Downloading graphviz-0.20.3-py3-none-any.whl.metadata (12 kB)\n",
            "Collecting tabulate>=0.7.7 (from eli5)\n",
            "  Downloading tabulate-0.9.0-py3-none-any.whl.metadata (34 kB)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\shyam\\appdata\\roaming\\python\\python312\\site-packages (from jinja2>=3.0.0->eli5) (3.0.2)\n",
            "Requirement already satisfied: joblib>=1.2.0 in c:\\users\\shyam\\appdata\\roaming\\python\\python312\\site-packages (from scikit-learn>=0.20->eli5) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\users\\shyam\\appdata\\roaming\\python\\python312\\site-packages (from scikit-learn>=0.20->eli5) (3.5.0)\n",
            "Downloading attrs-25.1.0-py3-none-any.whl (63 kB)\n",
            "Downloading tabulate-0.9.0-py3-none-any.whl (35 kB)\n",
            "Downloading graphviz-0.20.3-py3-none-any.whl (47 kB)\n",
            "Building wheels for collected packages: eli5\n",
            "  Building wheel for eli5 (pyproject.toml): started\n",
            "  Building wheel for eli5 (pyproject.toml): finished with status 'done'\n",
            "  Created wheel for eli5: filename=eli5-0.13.0-py2.py3-none-any.whl size=107822 sha256=ea13bef84af01c4aef3796e1b821bda43e41d14f9ef114fb6f8ad1feeac757f0\n",
            "  Stored in directory: c:\\users\\shyam\\appdata\\local\\pip\\cache\\wheels\\e0\\e4\\92\\b2763717e9a525c427c049221a2fa8fa4d0ec6190bfc587fc8\n",
            "Successfully built eli5\n",
            "Installing collected packages: tabulate, graphviz, attrs, eli5\n",
            "Successfully installed attrs-25.1.0 eli5-0.13.0 graphviz-0.20.3 tabulate-0.9.0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "[notice] A new release of pip is available: 24.3.1 -> 25.0\n",
            "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Defaulting to user installation because normal site-packages is not writeable\n",
            "Collecting scikit-learn==1.2.2\n",
            "  Downloading scikit-learn-1.2.2.tar.gz (7.3 MB)\n",
            "     ---------------------------------------- 0.0/7.3 MB ? eta -:--:--\n",
            "     ----- ---------------------------------- 1.0/7.3 MB 10.1 MB/s eta 0:00:01\n",
            "     -------------- ------------------------- 2.6/7.3 MB 8.9 MB/s eta 0:00:01\n",
            "     --------------------- ------------------ 3.9/7.3 MB 7.6 MB/s eta 0:00:01\n",
            "     ------------------------------ --------- 5.5/7.3 MB 7.3 MB/s eta 0:00:01\n",
            "     ------------------------------------- -- 6.8/7.3 MB 7.1 MB/s eta 0:00:01\n",
            "     ---------------------------------------- 7.3/7.3 MB 6.5 MB/s eta 0:00:00\n",
            "  Installing build dependencies: started\n",
            "  Installing build dependencies: still running...\n",
            "  Installing build dependencies: finished with status 'done'\n",
            "  Getting requirements to build wheel: started\n",
            "  Getting requirements to build wheel: finished with status 'done'\n",
            "  Preparing metadata (pyproject.toml): started\n",
            "  Preparing metadata (pyproject.toml): finished with status 'error'\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  error: subprocess-exited-with-error\n",
            "  \n",
            "  × Preparing metadata (pyproject.toml) did not run successfully.\n",
            "  │ exit code: 1\n",
            "  ╰─> [33 lines of output]\n",
            "      Partial import of sklearn during the build process.\n",
            "      Traceback (most recent call last):\n",
            "        File \"C:\\Users\\Shyam\\AppData\\Roaming\\Python\\Python312\\site-packages\\pip\\_vendor\\pyproject_hooks\\_in_process\\_in_process.py\", line 353, in <module>\n",
            "          main()\n",
            "        File \"C:\\Users\\Shyam\\AppData\\Roaming\\Python\\Python312\\site-packages\\pip\\_vendor\\pyproject_hooks\\_in_process\\_in_process.py\", line 335, in main\n",
            "          json_out['return_val'] = hook(**hook_input['kwargs'])\n",
            "                                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "        File \"C:\\Users\\Shyam\\AppData\\Roaming\\Python\\Python312\\site-packages\\pip\\_vendor\\pyproject_hooks\\_in_process\\_in_process.py\", line 149, in prepare_metadata_for_build_wheel\n",
            "          return hook(metadata_directory, config_settings)\n",
            "                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "        File \"C:\\Users\\Shyam\\AppData\\Local\\Temp\\pip-build-env-43m9ib45\\overlay\\Lib\\site-packages\\setuptools\\build_meta.py\", line 377, in prepare_metadata_for_build_wheel\n",
            "          self.run_setup()\n",
            "        File \"C:\\Users\\Shyam\\AppData\\Local\\Temp\\pip-build-env-43m9ib45\\overlay\\Lib\\site-packages\\setuptools\\build_meta.py\", line 522, in run_setup\n",
            "          super().run_setup(setup_script=setup_script)\n",
            "        File \"C:\\Users\\Shyam\\AppData\\Local\\Temp\\pip-build-env-43m9ib45\\overlay\\Lib\\site-packages\\setuptools\\build_meta.py\", line 320, in run_setup\n",
            "          exec(code, locals())\n",
            "        File \"<string>\", line 669, in <module>\n",
            "        File \"<string>\", line 663, in setup_package\n",
            "        File \"<string>\", line 597, in configure_extension_modules\n",
            "        File \"C:\\Users\\Shyam\\AppData\\Local\\Temp\\pip-install-nq73eo4f\\scikit-learn_61696aae61d4428988eb5933884bd054\\sklearn\\_build_utils\\__init__.py\", line 47, in cythonize_extensions\n",
            "          basic_check_build()\n",
            "        File \"C:\\Users\\Shyam\\AppData\\Local\\Temp\\pip-install-nq73eo4f\\scikit-learn_61696aae61d4428988eb5933884bd054\\sklearn\\_build_utils\\pre_build_helpers.py\", line 82, in basic_check_build\n",
            "          compile_test_program(code)\n",
            "        File \"C:\\Users\\Shyam\\AppData\\Local\\Temp\\pip-install-nq73eo4f\\scikit-learn_61696aae61d4428988eb5933884bd054\\sklearn\\_build_utils\\pre_build_helpers.py\", line 38, in compile_test_program\n",
            "          ccompiler.compile(\n",
            "        File \"C:\\Users\\Shyam\\AppData\\Local\\Temp\\pip-build-env-43m9ib45\\overlay\\Lib\\site-packages\\setuptools\\_distutils\\_msvccompiler.py\", line 379, in compile\n",
            "          self.initialize()\n",
            "        File \"C:\\Users\\Shyam\\AppData\\Local\\Temp\\pip-build-env-43m9ib45\\overlay\\Lib\\site-packages\\setuptools\\_distutils\\_msvccompiler.py\", line 289, in initialize\n",
            "          vc_env = _get_vc_env(plat_spec)\n",
            "                   ^^^^^^^^^^^^^^^^^^^^^^\n",
            "        File \"C:\\Users\\Shyam\\AppData\\Local\\Temp\\pip-build-env-43m9ib45\\overlay\\Lib\\site-packages\\setuptools\\_distutils\\_msvccompiler.py\", line 150, in _get_vc_env\n",
            "          raise DistutilsPlatformError(\n",
            "      distutils.errors.DistutilsPlatformError: Microsoft Visual C++ 14.0 or greater is required. Get it with \"Microsoft C++ Build Tools\": https://visualstudio.microsoft.com/visual-cpp-build-tools/\n",
            "      [end of output]\n",
            "  \n",
            "  note: This error originates from a subprocess, and is likely not a problem with pip.\n",
            "\n",
            "[notice] A new release of pip is available: 24.3.1 -> 25.0\n",
            "[notice] To update, run: python.exe -m pip install --upgrade pip\n",
            "error: metadata-generation-failed\n",
            "\n",
            "× Encountered error while generating package metadata.\n",
            "╰─> See above for output.\n",
            "\n",
            "note: This is an issue with the package mentioned above, not pip.\n",
            "hint: See above for details.\n"
          ]
        }
      ],
      "source": [
        "!pip install --upgrade eli5\n",
        "!pip install scikit-learn==1.2.2\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "gxD_3fs1tX_E"
      },
      "outputs": [
        {
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'distutils'",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[3], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01meli5\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mplt\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n",
            "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\eli5\\__init__.py:13\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mformatters\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m      7\u001b[0m     format_as_html,\n\u001b[0;32m      8\u001b[0m     format_html_styles,\n\u001b[0;32m      9\u001b[0m     format_as_text,\n\u001b[0;32m     10\u001b[0m     format_as_dict,\n\u001b[0;32m     11\u001b[0m )\n\u001b[0;32m     12\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mexplain\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m explain_weights, explain_prediction\n\u001b[1;32m---> 13\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msklearn\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m explain_weights_sklearn, explain_prediction_sklearn\n\u001b[0;32m     14\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtransform\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m transform_feature_names\n\u001b[0;32m     17\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
            "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\eli5\\sklearn\\__init__.py:3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# -*- coding: utf-8 -*-\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m__future__\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m absolute_import\n\u001b[1;32m----> 3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mexplain_weights\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m      4\u001b[0m     explain_weights_sklearn,\n\u001b[0;32m      5\u001b[0m     explain_linear_classifier_weights,\n\u001b[0;32m      6\u001b[0m     explain_linear_regressor_weights,\n\u001b[0;32m      7\u001b[0m     explain_rf_feature_importance,\n\u001b[0;32m      8\u001b[0m     explain_decision_tree,\n\u001b[0;32m      9\u001b[0m )\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mexplain_prediction\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m     11\u001b[0m     explain_prediction_sklearn,\n\u001b[0;32m     12\u001b[0m     explain_prediction_linear_classifier,\n\u001b[0;32m     13\u001b[0m     explain_prediction_linear_regressor,\n\u001b[0;32m     14\u001b[0m )\n\u001b[0;32m     15\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01munhashing\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m     16\u001b[0m     InvertableHashingVectorizer,\n\u001b[0;32m     17\u001b[0m     FeatureUnhasher,\n\u001b[0;32m     18\u001b[0m     invert_hashing_and_fit,\n\u001b[0;32m     19\u001b[0m )\n",
            "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\eli5\\sklearn\\explain_weights.py:64\u001b[0m\n\u001b[0;32m     62\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01meli5\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01munhashing\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m handle_hashing_vec, is_invhashing\n\u001b[0;32m     63\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01meli5\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtreeinspect\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m get_tree_info\n\u001b[1;32m---> 64\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01meli5\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m     65\u001b[0m     get_coef,\n\u001b[0;32m     66\u001b[0m     is_multiclass_classifier,\n\u001b[0;32m     67\u001b[0m     is_multitarget_regressor,\n\u001b[0;32m     68\u001b[0m     get_feature_names,\n\u001b[0;32m     69\u001b[0m     get_feature_names_filtered,\n\u001b[0;32m     70\u001b[0m     get_default_target_names,\n\u001b[0;32m     71\u001b[0m )\n\u001b[0;32m     72\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01meli5\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mexplain\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m explain_weights\n\u001b[0;32m     73\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01meli5\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtransform\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m transform_feature_names\n",
            "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\eli5\\sklearn\\utils.py:3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# -*- coding: utf-8 -*-\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m__future__\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m absolute_import\n\u001b[1;32m----> 3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mdistutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mversion\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m LooseVersion\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtyping\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Any, Optional, List, Tuple\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n",
            "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'distutils'"
          ]
        }
      ],
      "source": [
        "import eli5\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import sklearn\n",
        "from sklearn import datasets\n",
        "from sklearn.pipeline import FeatureUnion\n",
        "from sklearn.pipeline import Pipeline, make_pipeline\n",
        "from sklearn.base import BaseEstimator, TransformerMixin\n",
        "from sklearn.feature_extraction import DictVectorizer\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics import confusion_matrix, precision_score, precision_recall_curve, recall_score, f1_score\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.tree import DecisionTreeClassifier"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U0vF7uNdox7d"
      },
      "source": [
        "# Compare Logistic Regression and Decision Tree"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "orCxJPAWIxqk"
      },
      "source": [
        "## Prepare dataset and Pick two classes\n",
        "Your two classes should be similar, but opposite in some sense"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CPSdjUHDoSnf"
      },
      "outputs": [],
      "source": [
        "# categories = ['alt.atheism', 'soc.religion.christian']\n",
        "categories = ['comp.sys.ibm.pc.hardware', 'comp.sys.mac.hardware']\n",
        "# categories = ['rec.sport.baseball', 'rec.sport.hockey']\n",
        "# 'alt.atheism','comp.graphics','comp.os.ms-windows.misc','comp.sys.ibm.pc.hardware',\n",
        "# 'comp.sys.mac.hardware','comp.windows.x', 'misc.forsale', 'rec.autos',\n",
        "# 'rec.motorcycles',  'rec.sport.baseball', 'rec.sport.hockey', 'sci.crypt',\n",
        "# 'sci.electronics', 'sci.med', 'sci.space', 'soc.religion.christian', 'talk.politics.guns',\n",
        "# 'talk.politics.mideast', 'talk.politics.misc', 'talk.religion.misc'\n",
        "train = sklearn.datasets.fetch_20newsgroups(subset='train', categories=categories, remove=('headers', 'footers', 'quotes'),)\n",
        "test = sklearn.datasets.fetch_20newsgroups(subset='test', categories=categories, remove=('headers', 'footers', 'quotes'),)\n",
        "print('train data size:', len(train.data))\n",
        "print('test data size:', len(test.data))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u4mRc1xYI10a"
      },
      "source": [
        "## Compare Logistic Regression and Decision Tree models\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qiSoWKntytVh"
      },
      "outputs": [],
      "source": [
        "lr_model = LogisticRegression(C=1, solver='newton-cg')\n",
        "lr_features = CountVectorizer()\n",
        "lr_classifier = make_pipeline(lr_features, lr_model)\n",
        "lr_classifier.fit(train.data, train.target)\n",
        "\n",
        "dt_model = DecisionTreeClassifier(min_samples_split=0.4)\n",
        "dt_features = CountVectorizer()\n",
        "dt_classifier = make_pipeline(dt_features, dt_model)\n",
        "dt_classifier.fit(train.data, train.target)\n",
        "\n",
        "#Compare accuracy of the two models\n",
        "lr_train_preds = lr_classifier.predict(train.data)\n",
        "lr_train_f1 = f1_score(train.target, lr_train_preds, average='micro')\n",
        "lr_test_preds = lr_classifier.predict(test.data)\n",
        "lr_test_f1 = f1_score(test.target, lr_test_preds, average='micro')\n",
        "print(\"Train/test F1 for Logistic Regression: \", lr_train_f1, lr_test_f1)\n",
        "\n",
        "dt_train_preds = dt_classifier.predict(train.data)\n",
        "dt_train_f1 = f1_score(train.target, dt_train_preds, average='micro')\n",
        "dt_test_preds = dt_classifier.predict(test.data)\n",
        "dt_test_f1 = f1_score(test.target, dt_test_preds, average='micro')\n",
        "print(\"Train/test F1 for Decision Tree: \", dt_train_f1, dt_test_f1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fLzeRyqg55cD"
      },
      "outputs": [],
      "source": [
        "idx = 2\n",
        "x = test.data[idx]\n",
        "print(x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W8jgBhKroXXa"
      },
      "outputs": [],
      "source": [
        "idx = 2\n",
        "x = test.data[idx]\n",
        "print(test.target_names[test.target[idx]])\n",
        "eli5.show_prediction(lr_model, test.data[idx], vec=lr_features, target_names=test.target_names)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HzmPCJYO6VAo"
      },
      "outputs": [],
      "source": [
        "eli5.show_prediction(dt_model, test.data[idx], vec=dt_features, target_names=test.target_names)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rMwDXU314-qy"
      },
      "source": [
        "# Ensemble Methods"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O-uCwJdttlMR"
      },
      "outputs": [],
      "source": [
        "from sklearn.ensemble import VotingClassifier\n",
        "\n",
        "features = CountVectorizer()\n",
        "\n",
        "lr_model = LogisticRegression(C=1, solver='lbfgs')\n",
        "lr_classifier = make_pipeline(features, lr_model)\n",
        "lr_classifier.fit(train.data, train.target)\n",
        "\n",
        "#TODO FOR STUDENT: Try playing with the min_samples_split to see how it affect the ensemble score\n",
        "dt_model = DecisionTreeClassifier(min_samples_split=0.35)\n",
        "dt_classifier = make_pipeline(features, dt_model)\n",
        "dt_classifier.fit(train.data, train.target)\n",
        "\n",
        "#Compare accuracy of the two models\n",
        "lr_train_preds = lr_classifier.predict(train.data)\n",
        "lr_train_f1 = f1_score(train.target, lr_train_preds, average='micro')\n",
        "lr_test_preds = lr_classifier.predict(test.data)\n",
        "lr_test_f1 = f1_score(test.target, lr_test_preds, average='micro')\n",
        "print(\"Train/test F1 for Logistic Regression: \", lr_train_f1, lr_test_f1)\n",
        "\n",
        "dt_train_preds = dt_classifier.predict(train.data)\n",
        "dt_train_f1 = f1_score(train.target, dt_train_preds, average='micro')\n",
        "dt_test_preds = dt_classifier.predict(test.data)\n",
        "dt_test_f1 = f1_score(test.target, dt_test_preds, average='micro')\n",
        "print(\"Train/test F1 for Decision Tree: \", dt_train_f1, dt_test_f1)\n",
        "\n",
        "#Look at classifier agreement\n",
        "print(\"\\n% Cases where the two classifiers agree on test data: \", np.sum(lr_test_preds == dt_test_preds)/len(lr_test_preds))\n",
        "print(\"% Cases where one of the two classifiers has correct answer: \", np.sum(np.logical_or(lr_test_preds == test.target, dt_test_preds == test.target)/len(lr_test_preds)))\n",
        "\n",
        "#Try to build an ensemble combing both models\n",
        "#TODO FOR STUDENT: Modify the weights parameter which give different weight to each of the classifiers\n",
        "ensemble_classifier = make_pipeline(lr_features, VotingClassifier(estimators=[('lr', lr_model), ('dt', dt_model)], voting=\"soft\", weights=[1,1]))\n",
        "ensemble_classifier.fit(train.data, train.target)\n",
        "\n",
        "ensemble_train_preds = ensemble_classifier.predict(train.data)\n",
        "ensemble_train_f1 = f1_score(train.target, ensemble_train_preds, average='micro')\n",
        "ensemble_test_preds = ensemble_classifier.predict(test.data)\n",
        "ensemble_test_f1 = f1_score(test.target, ensemble_test_preds, average='micro')\n",
        "print(\"\\nTrain/test F1 for Ensemble: \", ensemble_train_f1, ensemble_test_f1)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LNVX3XIlTrpu"
      },
      "source": [
        "## Bagging"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sXWBquK2-ZDS"
      },
      "outputs": [],
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "#TODO FOR STUDENT: Try playing with n_estimators and min_samples_split\n",
        "ensemble_classifier = make_pipeline(lr_features, RandomForestClassifier(n_estimators=5, min_samples_split=0.3))\n",
        "ensemble_classifier.fit(train.data, train.target)\n",
        "\n",
        "ensemble_train_preds = ensemble_classifier.predict(train.data)\n",
        "ensemble_train_f1 = f1_score(train.target, ensemble_train_preds, average='micro')\n",
        "ensemble_test_preds = ensemble_classifier.predict(test.data)\n",
        "ensemble_test_f1 = f1_score(test.target, ensemble_test_preds, average='micro')\n",
        "print(\"\\nTrain/test F1 for Ensemble: \", ensemble_train_f1, ensemble_test_f1)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8s7XD4LlTrpu"
      },
      "source": [
        "## Boosting"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x9TMpBKOTrpv"
      },
      "outputs": [],
      "source": [
        "from sklearn.ensemble import AdaBoostClassifier\n",
        "ensemble_classifier = make_pipeline(lr_features, AdaBoostClassifier(n_estimators=50, learning_rate=0.5))\n",
        "ensemble_classifier.fit(train.data, train.target)\n",
        "\n",
        "ensemble_train_preds = ensemble_classifier.predict(train.data)\n",
        "ensemble_train_f1 = f1_score(train.target, ensemble_train_preds, average='micro')\n",
        "ensemble_test_preds = ensemble_classifier.predict(test.data)\n",
        "ensemble_test_f1 = f1_score(test.target, ensemble_test_preds, average='micro')\n",
        "print(\"\\nTrain/test F1 for Ensemble: \", ensemble_train_f1, ensemble_test_f1)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Wnizj2mlzqlo"
      },
      "outputs": [],
      "source": [
        "from sklearn.ensemble import GradientBoostingClassifier\n",
        "\n",
        "#TODO FOR STUDENT: Try playing with n_estimators and min_samples_split\n",
        "ensemble_classifier = make_pipeline(lr_features, GradientBoostingClassifier(n_estimators=50, min_samples_split=0.25))\n",
        "ensemble_classifier.fit(train.data, train.target)\n",
        "\n",
        "ensemble_train_preds = ensemble_classifier.predict(train.data)\n",
        "ensemble_train_f1 = f1_score(train.target, ensemble_train_preds, average='micro')\n",
        "ensemble_test_preds = ensemble_classifier.predict(test.data)\n",
        "ensemble_test_f1 = f1_score(test.target, ensemble_test_preds, average='micro')\n",
        "print(\"\\nTrain/test F1 for Ensemble: \", ensemble_train_f1, ensemble_test_f1)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fVuERhL4-avJ"
      },
      "outputs": [],
      "source": [
        "from xgboost import XGBClassifier\n",
        "\n",
        "#TODO FOR STUDENT: Try playing with n_estimators and min_samples_split\n",
        "ensemble_classifier = make_pipeline(lr_features, XGBClassifier(n_estimators=100, max_depth=10))\n",
        "ensemble_classifier.fit(train.data, train.target)\n",
        "\n",
        "ensemble_train_preds = ensemble_classifier.predict(train.data)\n",
        "ensemble_train_f1 = f1_score(train.target, ensemble_train_preds, average='micro')\n",
        "ensemble_test_preds = ensemble_classifier.predict(test.data)\n",
        "ensemble_test_f1 = f1_score(test.target, ensemble_test_preds, average='micro')\n",
        "print(\"\\nTrain/test F1 for Ensemble: \", ensemble_train_f1, ensemble_test_f1)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1AaRoNB8Trpv"
      },
      "outputs": [],
      "source": [
        "from sklearn.ensemble import GradientBoostingClassifier\n",
        "\n",
        "#TODO FOR STUDENT: Try playing with n_estimators and min_samples_split\n",
        "ensemble_classifier = make_pipeline(lr_features, GradientBoostingClassifier(n_estimators=100, min_samples_split=0.2))\n",
        "ensemble_classifier.fit(train.data, train.target)\n",
        "\n",
        "ensemble_train_preds = ensemble_classifier.predict(train.data)\n",
        "ensemble_train_f1 = f1_score(train.target, ensemble_train_preds, average='micro')\n",
        "ensemble_test_preds = ensemble_classifier.predict(test.data)\n",
        "ensemble_test_f1 = f1_score(test.target, ensemble_test_preds, average='micro')\n",
        "print(\"\\nTrain/test F1 for Ensemble: \", ensemble_train_f1, ensemble_test_f1)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p0DZif3U1ybt"
      },
      "source": [
        "# Comparing Bagging and Boosting"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tjZphbg60jTT"
      },
      "outputs": [],
      "source": [
        "for n_est in range(50,500,50):\n",
        "  ensemble_classifier = make_pipeline(lr_features, RandomForestClassifier(n_estimators=n_est, min_samples_split=0.008))\n",
        "  ensemble_classifier.fit(train.data, train.target)\n",
        "\n",
        "  ensemble_train_preds = ensemble_classifier.predict(train.data)\n",
        "  ensemble_train_f1 = f1_score(train.target, ensemble_train_preds, average='micro')\n",
        "  ensemble_test_preds = ensemble_classifier.predict(test.data)\n",
        "  ensemble_test_f1 = f1_score(test.target, ensemble_test_preds, average='micro')\n",
        "  print(n_est, \"Train/test F1 for Ensemble: \", ensemble_train_f1, ensemble_test_f1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hZcu4lzlSS-v"
      },
      "outputs": [],
      "source": [
        "ensemble_classifier = make_pipeline(lr_features, RandomForestClassifier(n_estimators=350, min_samples_split=0.25))\n",
        "print(n_est, \"Train/test F1 for Ensemble: \", ensemble_train_f1, ensemble_test_f1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "05wYMULj4qrN"
      },
      "outputs": [],
      "source": [
        "for n_est in range(10,200,15):\n",
        "  ensemble_classifier = make_pipeline(lr_features, RandomForestClassifier(n_estimators=n_est, min_samples_split=0.01))\n",
        "  ensemble_classifier.fit(train.data, train.target)\n",
        "\n",
        "  ensemble_train_preds = ensemble_classifier.predict(train.data)\n",
        "  ensemble_train_f1 = f1_score(train.target, ensemble_train_preds, average='micro')\n",
        "  ensemble_test_preds = ensemble_classifier.predict(test.data)\n",
        "  ensemble_test_f1 = f1_score(test.target, ensemble_test_preds, average='micro')\n",
        "  print(n_est, \"Train/test F1 for Ensemble: \", ensemble_train_f1, ensemble_test_f1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fJ6H4bzPzT2L"
      },
      "outputs": [],
      "source": [
        "for n_est in range(50,500,50):\n",
        "  ensemble_classifier = make_pipeline(lr_features, GradientBoostingClassifier(n_estimators=n_est, min_samples_split=0.05))\n",
        "  ensemble_classifier.fit(train.data, train.target)\n",
        "\n",
        "  ensemble_train_preds = ensemble_classifier.predict(train.data)\n",
        "  ensemble_train_f1 = f1_score(train.target, ensemble_train_preds, average='micro')\n",
        "  ensemble_test_preds = ensemble_classifier.predict(test.data)\n",
        "  ensemble_test_f1 = f1_score(test.target, ensemble_test_preds, average='micro')\n",
        "  print(n_est, \"Train/test F1 for Ensemble: \", ensemble_train_f1, ensemble_test_f1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JMts-TAFz48D"
      },
      "outputs": [],
      "source": [
        "for n_est in range(50,500,50):\n",
        "  ensemble_classifier = make_pipeline(lr_features, GradientBoostingClassifier(n_estimators=n_est, min_samples_split=0.5))\n",
        "  ensemble_classifier.fit(train.data, train.target)\n",
        "\n",
        "  ensemble_train_preds = ensemble_classifier.predict(train.data)\n",
        "  ensemble_train_f1 = f1_score(train.target, ensemble_train_preds, average='micro')\n",
        "  ensemble_test_preds = ensemble_classifier.predict(test.data)\n",
        "  ensemble_test_f1 = f1_score(test.target, ensemble_test_preds, average='micro')\n",
        "  print(n_est, \"Train/test F1 for Ensemble: \", ensemble_train_f1, ensemble_test_f1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R8XXdnR50UcS"
      },
      "outputs": [],
      "source": [
        "for n_est in range(50,500,50):\n",
        "  ensemble_classifier = make_pipeline(lr_features, GradientBoostingClassifier(n_estimators=n_est, min_samples_split=0.05))\n",
        "  ensemble_classifier.fit(train.data, train.target)\n",
        "\n",
        "  ensemble_train_preds = ensemble_classifier.predict(train.data)\n",
        "  ensemble_train_f1 = f1_score(train.target, ensemble_train_preds, average='micro')\n",
        "  ensemble_test_preds = ensemble_classifier.predict(test.data)\n",
        "  ensemble_test_f1 = f1_score(test.target, ensemble_test_preds, average='micro')\n",
        "  print(n_est, \"Train/test F1 for Ensemble: \", ensemble_train_f1, ensemble_test_f1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mef7kv2ZTPrE"
      },
      "outputs": [],
      "source": [
        "ensemble_classifier = make_pipeline(lr_features, GradientBoostingClassifier(n_estimators=400, min_samples_split=0.05))\n",
        "ensemble_classifier.fit(train.data, train.target)\n",
        "\n",
        "ensemble_train_preds = ensemble_classifier.predict(train.data)\n",
        "ensemble_train_f1 = f1_score(train.target, ensemble_train_preds, average='micro')\n",
        "ensemble_test_preds = ensemble_classifier.predict(test.data)\n",
        "ensemble_test_f1 = f1_score(test.target, ensemble_test_preds, average='micro')\n",
        "print(n_est, \"Train/test F1 for Ensemble: \", ensemble_train_f1, ensemble_test_f1)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
